在iOS中自动检测人说话结束是一个复杂的任务，需要结合音频处理和语音识别技术。一种方法是使用语音识别框架和音频处理来检测声音结束。
## 基于语音识别框架

1. **Speech框架：** iOS的Speech框架提供语音识别功能。你可以使用**SFSpeechRecognizer**进行语音识别，然后实时监测语音输入。
2. **音频录制和处理：** 使用**AVAudioRecorder**录制音频，然后实时分析音频数据，通过音频的振幅、能量等指标来判断是否有声音输入。当声音输入停止一段时间后，可以认为人停止说话。

这里有一个简单的示例，结合语音识别框架和音频处理的思路：
```swift
import UIKit
import Speech

class ViewController: UIViewController, SFSpeechRecognizerDelegate {

    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN")) // 设置语音识别语言环境
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()

    override func viewDidLoad() {
        super.viewDidLoad()
        speechRecognizer?.delegate = self

        SFSpeechRecognizer.requestAuthorization { authStatus in
            // 获取语音识别授权
            if authStatus == SFSpeechRecognizerAuthorizationStatus.authorized {
                self.startRecording() // 开始录音
            }
        }
    }

    func startRecording() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)

            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()

            let inputNode = audioEngine.inputNode
            guard let recognitionRequest = recognitionRequest else { fatalError("无法创建 SFSpeechAudioBufferRecognitionRequest 对象") }
            recognitionRequest.shouldReportPartialResults = true

            recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
                var isFinal = false
                if let result = result {
                    isFinal = result.isFinal
                    print("识别结果：\(result.bestTranscription.formattedString)")
                }

                if error != nil || isFinal {
                    self.audioEngine.stop()
                    inputNode.removeTap(onBus: 0)
                    self.recognitionRequest = nil
                    self.recognitionTask = nil
                    self.startRecording()
                }
            }

            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                self.recognitionRequest?.append(buffer)
            }

            audioEngine.prepare()
            try audioEngine.start()

        } catch {
            print("录音初始化失败")
        }
    }
}

```
这段代码通过Speech框架启动语音识别并开始录音，当识别到结果或出现错误时停止录音。你需要根据具体场景调整代码，并结合音频处理技术来优化识别结束的判断。
当使用 **SFSpeechRecognizer** 进行语音识别时，**isFinal** 的准确性会受到多种因素的影响，包括语音输入的长度、环境噪音、语音的清晰度等等。有时候 **isFinal** 可能会延迟或无法准确地反映语音识别是否结束。
你可以尝试以下方法改进准确性：

1. **调整识别请求参数：** 尝试调整识别请求的参数，如**SFSpeechRecognizer**的识别模式、音频源等，有时候这些参数的改变可能会提高准确性。
2. **设置合适的音频输入源：** 确保音频输入源清晰、稳定，避免环境噪音干扰。使用麦克风采集语音输入时，也需要确保麦克风的质量和位置是良好的。
3. **实时分析音频数据：** 可以通过分析音频波形、声音能量等指标来判断语音是否结束，而不完全依赖于 **isFinal** 的返回。这需要对音频处理有一定的了解。
4. **处理语音识别的错误或超时情况：** 在 **SFSpeechRecognitionTask** 的回调中，及时处理识别的错误情况或超时情况，及时结束识别任务并重新开始。

以上方法可能需要根据具体情况进行调整和优化。语音识别技术本身也在不断发展，可能随着时间的推移，会有更加准确和稳定的解决方案出现。
**或者在识别文字之后三秒内无新增，则说明说话结束，停止录音即可。**
## 基于音频处理方法

1. **声音分析：** 使用音频处理库对录制的音频进行声音分析。当检测到一段时间内声音强度低于一定阈值时，可以认为人停止说话。
2. **静音检测：** 通过分析音频波形的变化或能量水平的变化来检测静音部分，从而判断说话是否结束。

     无论选择哪种方法，都需要考虑到环境噪音、语音特征等因素。这些方法可能需要一定的算法和处理技术，建议在实际应用中结合调试和测试以获得更好的效果。

   要从音频波形和声音能量等指标来判断语音是否结束，你可以使用音频处理库来分析音频数据。以下是一个示例，展示了如何使用音频处理库**AVAudioEngine**来获取音频波形数据，并基于音频能量判断语音是否结束：
```swift
import UIKit
import AVFoundation

class ViewController: UIViewController {

    var audioEngine = AVAudioEngine()
    var audioPlayerNode = AVAudioPlayerNode()
    var audioFile: AVAudioFile!
    
    override func viewDidLoad() {
        super.viewDidLoad()
        startAudioEngine()
    }
    
    func startAudioEngine() {
        let audioSession = AVAudioSession.sharedInstance()
        
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            let audioURL = Bundle.main.url(forResource: "your_audio_file", withExtension: "mp3") // 替换成你的音频文件
            audioFile = try AVAudioFile(forReading: audioURL!)
            
            let format = audioFile.processingFormat
            let frameLength = UInt32(audioFile.length)
            let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameLength)!
            try audioFile.read(into: buffer)
            
            audioEngine.attach(audioPlayerNode)
            audioEngine.connect(audioPlayerNode, to: audioEngine.mainMixerNode, format: format)
            
            audioEngine.prepare()
            try audioEngine.start()
            
            audioPlayerNode.scheduleBuffer(buffer, at: nil, options: .loops, completionHandler: nil)
            audioPlayerNode.play()
            
            // 开始分析音频数据
            analyzeAudio()
            
        } catch {
            print("Error starting audio engine: \(error.localizedDescription)")
        }
    }
    
    func analyzeAudio() {
        let format = audioFile.processingFormat
        let frameLength = UInt32(audioFile.length)
        let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameLength)!
        try! audioFile.read(into: buffer)
        
        let audioBuffer = buffer.floatChannelData![0]
        let frameCount = buffer.frameLength
        
        var totalPower: Float = 0.0
        
        for frame in 0..<Int(frameCount) {
            let power = audioBuffer[frame] * audioBuffer[frame] // 计算音频数据的能量
            totalPower += power
        }
        
        let averagePower = totalPower / Float(frameCount)
        // 根据音频能量判断语音是否结束
        if averagePower < 0.001 {
            // 语音结束
            print("语音结束")
        } else {
            // 继续分析音频数据或执行其他操作
            analyzeAudio()
        }
    }
}

```
这段代码展示了如何使用AVAudioEngine、AVAudioFile和音频缓冲区来读取音频数据，并计算音频数据的能量。你可以根据实际情况调整阈值和处理逻辑来判断语音是否结束。
## 开源方案
**FDSoundActivatedRecorder**是一个基于声音激活的音频录制库，可以用于自动检测语音结束。它能够在检测到声音时开始录制，并在声音停止一段时间后自动停止录制。你可以在录制开始和结束时执行相应的操作，比如保存录音或执行其他逻辑。
以下是一个简单的示例，展示了如何使用**FDSoundActivatedRecorder**库：
首先，确保你已经在项目中集成了**FDSoundActivatedRecorder**库。
```swift
import UIKit
import FDSoundActivatedRecorder

class ViewController: UIViewController {

    var recorder: FDSoundActivatedRecorder!
    
    override func viewDidLoad() {
        super.viewDidLoad()
        setupRecorder()
    }

    func setupRecorder() {
        recorder = FDSoundActivatedRecorder()
        recorder.delegate = self
        recorder.startListening()
    }
}

extension ViewController: FDSoundActivatedRecorderDelegate {
    func soundActivatedRecorderDidStartRecording(_ recorder: FDSoundActivatedRecorder) {
        print("开始录音")
    }

    func soundActivatedRecorderDidFinishRecording(_ recorder: FDSoundActivatedRecorder) {
        print("停止录音")
        let recordedAudioURL = recorder.recordedURL()
        // 可以在这里处理录音文件，比如保存或播放
    }
}

```
在这个示例中，我们创建了一个**FDSoundActivatedRecorder**的实例，并设置了代理方法来监听录音的开始和结束事件。当检测到声音时，会调用 **soundActivatedRecorderDidStartRecording** 方法开始录音，当声音停止一段时间后，会调用 **soundActivatedRecorderDidFinishRecording** 方法停止录音。你可以在这些方法中执行相应的操作来处理录音文件

自动检测语音结束的逻辑通常需要结合声音激活的录制功能以及一定的逻辑判断。

1. **声音激活录制：** 使用声音激活录制库（比如**FDSoundActivatedRecorder**），当检测到声音时开始录制音频，当一段时间内没有声音输入时停止录制。这个过程一般会自动触发，不需要手动操作。
2. **声音停止判断：** 当录制开始后，你可以通过设定一个时间阈值或者声音能量的阈值来判断声音是否停止。例如，在检测到声音后开始计时，如果一段时间内（比如3秒钟）声音能量低于设定的阈值，则判定语音结束。
```swift
extension ViewController: FDSoundActivatedRecorderDelegate {
    func soundActivatedRecorderDidStartRecording(_ recorder: FDSoundActivatedRecorder) {
        print("开始录音")
        // 在此处开始计时或记录开始录音的时间戳
    }

    func soundActivatedRecorderDidFinishRecording(_ recorder: FDSoundActivatedRecorder) {
        print("停止录音")
        let recordedAudioURL = recorder.recordedURL()
        // 可以在这里处理录音文件，比如保存或播放
    }

    func soundActivatedRecorder(_ recorder: FDSoundActivatedRecorder, belowLevel level: Float) {
        // 当声音激活时的回调，在此处判断声音能量是否低于阈值
        if level < 0.1 { // 举例，你需要根据实际情况设置阈值
            recorder.finishRecording() // 当声音能量低于阈值时，结束录音
        }
    }
}

```
这段代码使用 **FDSoundActivatedRecorder** 库来自动开始和结束录音，同时在声音激活时监测声音能量是否低于阈值，当声音停止时结束录音。请根据实际需求调整阈值和处理逻辑。
****FDSoundActivatedRecorder**提供了声音激活录制的功能，并在声音激活后开始录制，当一定时间内未检测到声音或声音能量低于阈值时停止录制。这个库的实现方案是有效的，并且基本满足自动检测语音结束的需求。
### 优点：

1. **声音激活录制：** 声音激活录制功能有效地捕捉到声音开始，并启动录音功能。
2. **自动检测声音结束：** 在未检测到声音或声音能量低于阈值时停止录音，实现了自动检测语音结束的功能。
### 需要注意的地方：

1. **阈值设定：** 需要根据具体的应用场景和环境噪音情况来合理设置声音能量的阈值。不同环境下，声音的能量变化可能会有所不同，因此需要根据实际情况进行调整。
2. **灵敏度问题：** 有时阈值设置过高或过低可能导致录音的过早结束或持续录制。需要综合考虑灵敏度和实际情况做出调整。

总的来说，**FDSoundActivatedRecorder**提供了一种便捷的方法来实现语音激活的录制功能，并且对于许多简单的语音录制场景是有效的。但在复杂环境或特殊需求下，可能需要更精细的参数调整或其他技术方案来满足特定的需求。
